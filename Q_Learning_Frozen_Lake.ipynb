{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gamma = 0.9\\nalpha = 0.85\\nepsilon = 0.8\\nepisodes = 1000\\ntime_limit = 100'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "def epsilon_greedy(env,state,q,epsilon):\n",
    "    if np.random.default_rng().random() < epsilon : \n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        \n",
    "        return max(list(range(env.action_space.n)),key = lambda x: q[(state,x)])\n",
    "    \n",
    "def q_learning(env,episodes,time_limit,gamma,epsilon,alpha,epsilon_decay):\n",
    "    q = np.zeros((env.observation_space.n,env.action_space.n),dtype=float)\n",
    "\n",
    "    for i in range(episodes):\n",
    "        state , _ = env.reset()\n",
    "        \n",
    "        truncated = False\n",
    "        terminated = False\n",
    "        time_step = 0\n",
    "\n",
    "        while not terminated and not truncated:\n",
    "            action = epsilon_greedy(env,state,q,epsilon)\n",
    "            next_state , reward , terminated , truncated , _ = env.step(action)\n",
    "\n",
    "            next_action = np.argmax([q[(next_state,a)] for a in range(env.action_space.n)])\n",
    "            q[(state,action)] += alpha * (reward + gamma * q[(next_state,next_action)] - q[(state , action)])\n",
    "            \n",
    "            state = next_state\n",
    "\n",
    "            time_step += 1 \n",
    "            if time_step > time_limit:\n",
    "                truncated = True\n",
    "                \n",
    "        if epsilon > 0 and epsilon_decay :\n",
    "            epsilon -= 0.001\n",
    "\n",
    "    return q\n",
    "\n",
    "\n",
    "def walk(q,slippery=False):\n",
    "    env = gym.make(\"FrozenLake-v1\" , map_name='4x4' , is_slippery=slippery , render_mode='human')\n",
    "    state,_ = env.reset()\n",
    "    terminated = False\n",
    "    step = 60\n",
    "    while not terminated:\n",
    "        max = -10000000000\n",
    "        for a in range(env.action_space.n):\n",
    "            if max < q[(state,a)]:\n",
    "                max = q[(state,a)]\n",
    "                action = a\n",
    "\n",
    "        next_state , reward, terminated, truncated , _ = env.step(action)\n",
    "        state = next_state \n",
    "        step-=1\n",
    "        if step == 0 : break\n",
    "    env.close()\n",
    "\n",
    "\n",
    "'''gamma = 0.9\n",
    "alpha = 0.85\n",
    "epsilon = 0.8\n",
    "episodes = 1000\n",
    "time_limit = 100'''\n",
    "# alpha = 0.85\n",
    "# gamma = 0.9\n",
    "# epsilon = 0.8\n",
    "# episodes = 1000\n",
    "# time_limit = 100\n",
    "\n",
    "# env = gym.make(\"FrozenLake-v1\" , map_name='4x4' , is_slippery=True , render_mode=None)\n",
    "\n",
    "# q = np.zeros((env.observation_space.n,env.action_space.n),dtype=float)\n",
    "\n",
    "# q_values = q_learning(env,episodes,time_limit,gamma,epsilon,alpha,epsilon_decay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.33345154e-02, 9.84270753e-04, 1.30005900e-03, 7.79550572e-04],\n",
       "       [3.55274825e-04, 2.47631389e-04, 2.85611635e-04, 1.44433374e-03],\n",
       "       [4.28712867e-04, 4.66261161e-04, 3.17735820e-03, 4.60279233e-04],\n",
       "       [2.05712702e-04, 1.66214305e-04, 1.31019839e-04, 5.45604661e-03],\n",
       "       [5.02843336e-02, 1.07617552e-04, 7.40825621e-04, 6.42598683e-04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [2.04802873e-03, 4.67986035e-05, 2.53090699e-05, 1.85882332e-05],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [9.81019299e-04, 1.52882979e-03, 3.74836791e-03, 6.18989115e-02],\n",
       "       [2.52836507e-03, 4.71169060e-02, 2.76501074e-03, 2.00624264e-03],\n",
       "       [1.22033663e-03, 2.13310533e-01, 3.77552153e-04, 1.49708728e-03],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [4.21334549e-02, 1.48581361e-02, 1.38571926e-01, 4.94603650e-02],\n",
       "       [5.78133267e-02, 8.72364481e-01, 2.52190990e-02, 1.82009084e-02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(time_limit,episodes,epsilon,gamma,alpha,slippery):\n",
    "    env = gym.make(\"FrozenLake-v1\" , map_name='4x4' , is_slippery=slippery , render_mode=None)\n",
    "    q = np.zeros((env.observation_space.n,env.action_space.n),dtype=float)\n",
    "    q_values = q_learning(env,episodes,time_limit,gamma,epsilon,alpha,epsilon_decay=True)\n",
    "    walk(q_values,slippery)\n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q_Learning non Slippery mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.53085328, 0.59049   , 0.473513  , 0.53104104],\n",
       "       [0.53020294, 0.        , 0.23942438, 0.2667776 ],\n",
       "       [0.07583213, 0.50806715, 0.00939939, 0.12968588],\n",
       "       [0.07859892, 0.        , 0.0013253 , 0.00497978],\n",
       "       [0.58860323, 0.6561    , 0.        , 0.53089305],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.78584862, 0.        , 0.17062759],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.65476163, 0.        , 0.729     , 0.58872523],\n",
       "       [0.64796636, 0.80011288, 0.81      , 0.        ],\n",
       "       [0.72534163, 0.9       , 0.        , 0.64880269],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.4210743 , 0.89943301, 0.52957274],\n",
       "       [0.7771445 , 0.88703079, 1.        , 0.80424855],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(time_limit=100,episodes=1000,epsilon=1,gamma=0.9,alpha=0.1,slippery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.531441, 0.59049 , 0.59049 , 0.531441],\n",
       "       [0.531441, 0.      , 0.6561  , 0.59049 ],\n",
       "       [0.59049 , 0.729   , 0.59049 , 0.6561  ],\n",
       "       [0.6561  , 0.      , 0.59049 , 0.      ],\n",
       "       [0.59049 , 0.6561  , 0.      , 0.531441],\n",
       "       [0.      , 0.      , 0.      , 0.      ],\n",
       "       [0.      , 0.81    , 0.      , 0.6561  ],\n",
       "       [0.      , 0.      , 0.      , 0.      ],\n",
       "       [0.6561  , 0.      , 0.729   , 0.59049 ],\n",
       "       [0.6561  , 0.81    , 0.81    , 0.      ],\n",
       "       [0.729   , 0.9     , 0.      , 0.729   ],\n",
       "       [0.      , 0.      , 0.      , 0.      ],\n",
       "       [0.      , 0.      , 0.      , 0.      ],\n",
       "       [0.      , 0.81    , 0.9     , 0.729   ],\n",
       "       [0.81    , 0.9     , 1.      , 0.81    ],\n",
       "       [0.      , 0.      , 0.      , 0.      ]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(time_limit=100,episodes=1000,epsilon=1,gamma=0.9,alpha=1,slippery=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that alpha ais controling the size of the steps that change our values. for this small environment we can set different alpha's and still converge to optimal value but for more complex problems we must select a proper alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.98693031e-07, 1.00000000e-05, 9.65482246e-08, 9.96993041e-07],\n",
       "       [9.92679084e-07, 0.00000000e+00, 1.20579952e-09, 3.75904845e-08],\n",
       "       [4.36461161e-08, 0.00000000e+00, 0.00000000e+00, 3.01431362e-11],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [9.99592514e-06, 1.00000000e-04, 0.00000000e+00, 9.95401190e-07],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [9.95411973e-05, 0.00000000e+00, 1.00000000e-03, 9.96096312e-06],\n",
       "       [9.63821129e-05, 1.00000000e-02, 9.74177957e-03, 0.00000000e+00],\n",
       "       [5.85662346e-04, 9.97770348e-02, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 9.69395673e-03, 1.00000000e-01, 9.77518425e-04],\n",
       "       [9.23504145e-03, 9.83459012e-02, 1.00000000e+00, 9.21984861e-03],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(time_limit=100,episodes=1000,epsilon=1,gamma=0.1,alpha=0.1,slippery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01561509, 0.03125   , 0.03033214, 0.01561929],\n",
       "       [0.01036802, 0.        , 0.06177317, 0.02359898],\n",
       "       [0.01817794, 0.12466852, 0.01330744, 0.04532952],\n",
       "       [0.04300177, 0.        , 0.00163368, 0.00207089],\n",
       "       [0.03120907, 0.0625    , 0.        , 0.0155646 ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.24995956, 0.        , 0.03794538],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.06235241, 0.        , 0.125     , 0.03117891],\n",
       "       [0.06186879, 0.24896095, 0.25      , 0.        ],\n",
       "       [0.12242441, 0.5       , 0.        , 0.12355043],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.19608944, 0.4999765 , 0.1002378 ],\n",
       "       [0.24883144, 0.49656493, 1.        , 0.24925495],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(time_limit=100,episodes=1000,epsilon=1,gamma=0.5,alpha=0.1,slippery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.53010632, 0.59049   , 0.51466908, 0.53098799],\n",
       "       [0.27368609, 0.        , 0.60929532, 0.25634861],\n",
       "       [0.24053257, 0.7075416 , 0.10036788, 0.33102781],\n",
       "       [0.27403454, 0.        , 0.03856533, 0.04887047],\n",
       "       [0.59011765, 0.6561    , 0.        , 0.53046742],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.80445458, 0.        , 0.24656063],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.65421488, 0.        , 0.729     , 0.58801171],\n",
       "       [0.6479249 , 0.81      , 0.80254316, 0.        ],\n",
       "       [0.56588772, 0.89995229, 0.        , 0.52281443],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.80145851, 0.9       , 0.66221181],\n",
       "       [0.79393241, 0.8886933 , 1.        , 0.77494805],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(time_limit=100,episodes=1000,epsilon=1,gamma=0.9,alpha=0.1,slippery=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see with higher value of gamma we will look further into future and also it is controling the magnitude of our values or we could say with higher gammas , large rewards from future play a rule in the values of the current states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q_Learning Slippery mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02149549, 0.07185334, 0.02168671, 0.0215317 ],\n",
       "       [0.01986832, 0.01955907, 0.02006845, 0.08067461],\n",
       "       [0.09767961, 0.02443553, 0.02433254, 0.02429125],\n",
       "       [0.00332768, 0.00556823, 0.00136335, 0.01987807],\n",
       "       [0.07934538, 0.02522411, 0.0249299 , 0.02297341],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.02082503, 0.02043257, 0.156144  , 0.00589448],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.03724237, 0.03611819, 0.03854732, 0.14363232],\n",
       "       [0.09432223, 0.20674807, 0.09705156, 0.09651091],\n",
       "       [0.3059019 , 0.08385382, 0.07961266, 0.04013823],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.14928742, 0.16488871, 0.42009273, 0.16590303],\n",
       "       [0.3188797 , 0.69643665, 0.36217873, 0.36863721],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(time_limit=100,episodes=10000,epsilon=1,gamma=0.9,alpha=0.1,slippery=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.62758366e-02, 5.83027769e-02, 5.82558651e-02, 4.73438879e-02],\n",
       "       [3.09125835e-02, 2.08066167e-02, 1.69840450e-02, 5.30398863e-02],\n",
       "       [4.57465565e-02, 2.00040104e-02, 2.26253868e-02, 1.52784418e-02],\n",
       "       [4.33046468e-03, 6.68188788e-03, 3.63059565e-03, 1.86852787e-02],\n",
       "       [1.01109778e-01, 5.89469407e-02, 4.95351107e-02, 3.69374739e-02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [4.19608184e-02, 2.00507464e-02, 8.40278947e-02, 2.17374946e-04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [5.68289409e-02, 6.69057920e-02, 6.02875898e-02, 1.64650554e-01],\n",
       "       [1.12197266e-01, 2.84483458e-01, 1.36207219e-01, 7.73811274e-02],\n",
       "       [3.24019172e-01, 1.65032677e-01, 1.29237181e-01, 2.55499796e-02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.35882693e-01, 2.69349412e-01, 3.83928501e-01, 2.26768767e-01],\n",
       "       [2.67491343e-01, 3.88604156e-01, 5.97741065e-01, 4.20396150e-01],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(time_limit=100,episodes=1000,epsilon=1,gamma=0.9,alpha=0.1,slippery=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06701949, 0.02967171, 0.02949263, 0.02943034],\n",
       "       [0.01575162, 0.02545348, 0.02382608, 0.06583942],\n",
       "       [0.08237864, 0.02637466, 0.0270461 , 0.02236167],\n",
       "       [0.00991202, 0.01356474, 0.00100929, 0.00667574],\n",
       "       [0.09501429, 0.03425555, 0.02777407, 0.0333313 ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.0225001 , 0.0163727 , 0.14508225, 0.01139314],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.03813586, 0.03780996, 0.04493047, 0.1469175 ],\n",
       "       [0.07709792, 0.33193565, 0.07360744, 0.07487169],\n",
       "       [0.34638048, 0.08493054, 0.11102589, 0.05023311],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.0941465 , 0.16671062, 0.54152491, 0.15828837],\n",
       "       [0.14113118, 0.76842951, 0.31366591, 0.25601451],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(time_limit=100,episodes=50000,epsilon=1,gamma=0.9,alpha=0.1,slippery=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we suspected adding stochastic properties to environment , make the problem more complex . in the results we can see Q_learning can find the optimal q_values but it need more time(episodes) and hyperparameters must be selected with more precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double Q_learning vs Q_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance : both Q_learning and Double Q_learning perfrom well and converge to optimal policy . \n",
    "Convergence Speed : Q_learning is fast and stable but Double q_learning is slightly slower due to the need to update two separate value functions.\n",
    "Maxization Bias : the main reason of using Double Q_learning was to control maximization bias and it fix the problem . \n",
    "\n",
    "But in the slippery mode we see some changes in the performance of this algorithms. \n",
    "Q_learning performance degrade and become slower and unstable while Double Q_learning show improved speed and performance due to reducing overestimation bias."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
