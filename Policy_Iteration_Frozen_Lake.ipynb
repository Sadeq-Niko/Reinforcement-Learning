{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env,policy,num_iteration,theta,gamma):\n",
    "    value = np.zeros(env.observation_space.n)\n",
    "    for i in range(num_iteration):\n",
    "        updated_value = np.copy(value)\n",
    "        for state in range(env.observation_space.n):\n",
    "            action = policy[state]\n",
    "            value[state] = sum(reward + probability * gamma * updated_value[next_state] for probability , next_state , reward, _ in env.P[state][action])\n",
    "        \n",
    "        diff = np.sum(np.abs(np.subtract(updated_value , value))) \n",
    "        if diff <= theta:\n",
    "            break\n",
    "    \n",
    "    policy = np.zeros(env.observation_space.n,dtype=int)\n",
    "    for state in range(env.observation_space.n):\n",
    "        q_values = [sum((reward + probability * gamma * value[next_state]) for probability , next_state, reward, _ in env.P[state][action]) for action in range(env.action_space.n)]\n",
    "        policy[state] = np.argmax(q_values)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env,num_iteration,theta,gamma):\n",
    "    policy = np.zeros(env.observation_space.n,dtype=int)\n",
    "    for i in range(num_iteration):\n",
    "        new_policy = policy_evaluation(env,policy,num_iteration,theta,gamma)\n",
    "        # new_policy = extract_policy(value,gamma)\n",
    "        if (np.all(policy==new_policy)):\n",
    "            break\n",
    "        else:\n",
    "            policy = new_policy\n",
    "            \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_iteration,theta,gamma,slippery):\n",
    "    env = gym.make(\"FrozenLake-v1\",map_name='4x4',render_mode=None,is_slippery=slippery)\n",
    "    optimal_policy = policy_iteration(env,num_iteration,theta,gamma)\n",
    "    return optimal_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk(policy,slippery):\n",
    "    env = gym.make(\"FrozenLake-v1\",map_name='4x4',render_mode='human',is_slippery=slippery)\n",
    "    state = env.reset()\n",
    "    terminated = False\n",
    "    state = 0\n",
    "    while not terminated:\n",
    "        action = policy[state]\n",
    "        next_state , reward, terminated, truncated , _ = env.step(action)\n",
    "        state = next_state \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = train(num_iteration=6,theta=0.1,gamma=0.5,slippery=False)\n",
    "walk(policy,slippery=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Iteration in non slippery mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that with appropriate hyperparameters all gamma rates converge to same policy . But the difference is in the speed of convergence and also we will see the difference of this gammas in more complex environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low gamma\n",
    "gamma = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 1 0 1 0 1 0 2 1 1 0 0 2 2 0]\n"
     ]
    }
   ],
   "source": [
    "policy = train(num_iteration=6,theta=0.001,gamma=0.1,slippery=False)\n",
    "print(policy)\n",
    "walk(policy,slippery=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that with proper theta(0.001) wich is less that mid and high gamma appropriate theta we can converge to optimal policy . The optimal policy highly focus on immediate rewards (would be more obvious in environments with bigger reward range).\n",
    "We can say that the low gamma converge quickly but might result in suboptimal policies beacuse it priotrizes immediate rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mid gamma\n",
    "gamma = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 1 0 1 0 1 0 2 1 1 0 0 2 2 0]\n"
     ]
    }
   ],
   "source": [
    "policy = train(num_iteration=6,theta=0.1,gamma=0.5,slippery=False)\n",
    "print(policy)\n",
    "walk(policy,slippery=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did get the same optimal policy in this env , but the theta could be much bigger  . \n",
    "We can say that the mid gamma convergence is stable and it consider a balance between imediate and discounted rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High gama\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 1 0 1 0 1 0 2 1 1 0 0 2 2 0]\n"
     ]
    }
   ],
   "source": [
    "policy = train(num_iteration=6,theta=0.9,gamma=0.9,slippery=False)\n",
    "print(policy)\n",
    "walk(policy,slippery=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For higher gamma we have the same optimal policy but with much higher theta . We can say that high gamma converge slower but in the complex environments the policy will be more optimal , beacuse it consideres long term rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Iteration in slippery mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to learn in a stochastic environment is more challenging . We can see the algorithm try to avoid non goal terminal states and sometimes it even use the randomness of the environment as a tool ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 0 3 0 0 0 0 3 1 0 0 0 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "policy = train(num_iteration=6,theta=0.0001,gamma=0.1,slippery=True)\n",
    "print(policy)\n",
    "walk(policy,slippery=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the slippery property , we see the need to use more accurate values by using smaller theta . everything that we said about low gamma in non slippery part also holds here . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mid gamma\n",
    "gamma = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 2 3 0 0 0 0 3 1 0 0 0 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "policy = train(num_iteration=6,theta=0.001,gamma=0.5,slippery=True)\n",
    "print(policy)\n",
    "walk(policy,slippery=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see an interesting act of policy wich is in state before goal . we can see that the policy choose  the down action instead of going right wich give agent the chance to slipp right to the goal ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High gama\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 2 3 0 0 0 0 3 1 0 0 0 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "policy = train(num_iteration=6,theta=0.001,gamma=0.9,slippery=True)\n",
    "print(policy)\n",
    "walk(policy,slippery=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The high gamma learn the exact policy as mid gamma due to simple environment and also some similar attributes of both mid and high gamma(both consider future rewards but with different weights) . \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Iteration vs Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance of both algorithms varies based on the discount factor and the evironment's stochatic property . \n",
    "Policy iteration is faster due to variating between policies . \n",
    "Value iteration seems to be more robust in stochastic environment . "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
