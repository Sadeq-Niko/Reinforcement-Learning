{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "def epsilon_greedy(state,epsilon):\n",
    "    if np.random.default_rng().random() < epsilon : \n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return max(list(range(env.action_space.n)),key = lambda x: q[(state,x)])\n",
    "    \n",
    "def sarsa(env,episodes,time_limit,gamma,epsilon,alpha,epsilon_decay):\n",
    "        \n",
    "    for i in range(episodes):\n",
    "\n",
    "        state , _= env.reset()\n",
    "\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        time_step = 0\n",
    "        \n",
    "        action = epsilon_greedy(state,epsilon)\n",
    "        while not terminated and not truncated:\n",
    "            next_state,reward,terminated,truncated,_=env.step(action)\n",
    "            next_action = epsilon_greedy(next_state,epsilon)\n",
    "    \n",
    "            q[(state,action)] += alpha * (reward + gamma*  q[(next_state,next_action)] -  q[(state,action)])\n",
    "            \n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            time_step +=1 \n",
    "            if time_step > time_limit: \n",
    "                truncated = True\n",
    "            \n",
    "        if epsilon > 0 and epsilon_decay :\n",
    "            epsilon -= 0.001\n",
    "                \n",
    "    return q\n",
    "\n",
    "def walk(q):\n",
    "    env = gym.make('CliffWalking-v0', render_mode='human')\n",
    "    state,_ = env.reset()\n",
    "    terminated = False\n",
    "    step = 60\n",
    "    while not terminated:\n",
    "        max = -10000000000\n",
    "        for a in range(env.action_space.n):\n",
    "            if max < q[(state,a)]:\n",
    "                max = q[(state,a)]\n",
    "                action = a\n",
    "        next_state , reward, terminated, truncated , _ = env.step(action)\n",
    "        state = next_state \n",
    "        step-=1\n",
    "        if step == 0 : break\n",
    "    env.close()\n",
    "    \n",
    "''' ALPHA = 0.1\n",
    "    GAMMA = 0.9    \n",
    "    EPS = 1.0'''\n",
    "\n",
    "gamma = 0.9\n",
    "alpha = 0.1\n",
    "epsilon = 0.1\n",
    "episodes = 5000\n",
    "time_limit = 100\n",
    "\n",
    "env = gym.make('CliffWalking-v0', render_mode=None)\n",
    "\n",
    "q = np.zeros((env.observation_space.n,env.action_space.n),dtype=float)\n",
    "\n",
    "q_ = sarsa(env,episodes,time_limit,gamma,epsilon,alpha,epsilon_decay=False)\n",
    "# walk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk(q_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  -8.2302304 ,   -8.08240054,   -8.46284637,   -8.20934496],\n",
       "       [  -8.03098178,   -7.80356035,   -8.2841103 ,   -8.2670466 ],\n",
       "       [  -7.75036676,   -7.52862753,   -7.95352876,   -8.05082274],\n",
       "       [  -7.50154443,   -7.26383359,   -7.69563146,   -7.8112362 ],\n",
       "       [  -7.20937638,   -6.91681403,   -7.45443767,   -7.56398334],\n",
       "       [  -6.84516672,   -6.54778572,   -6.66741245,   -7.31530633],\n",
       "       [  -6.46020979,   -6.15042765,   -6.27773348,   -6.94180078],\n",
       "       [  -6.1064504 ,   -5.61084367,   -5.80961343,   -6.52926486],\n",
       "       [  -5.54736778,   -5.05456686,   -5.33197772,   -6.0965507 ],\n",
       "       [  -5.06604993,   -4.46273261,   -4.77435057,   -5.67782368],\n",
       "       [  -4.499018  ,   -3.72225209,   -4.02148909,   -5.10257132],\n",
       "       [  -3.70339019,   -3.71777038,   -2.86448663,   -4.448306  ],\n",
       "       [  -8.26485326,   -8.32180621,   -8.69556374,   -8.4414277 ],\n",
       "       [  -8.03380013,   -8.34383368,  -13.11223297,   -8.35021995],\n",
       "       [  -7.74233526,   -7.73109683,  -12.99766752,   -8.07055632],\n",
       "       [  -7.50525179,   -7.51179388,   -7.60223513,   -7.82827843],\n",
       "       [  -7.22052363,   -7.24606487,   -7.28238814,   -7.48929794],\n",
       "       [  -6.81198735,   -6.27562199,  -12.48429077,   -7.14296621],\n",
       "       [  -6.42405779,   -5.81738778,   -6.35310985,   -6.71928682],\n",
       "       [  -6.00893067,   -5.18429223,   -6.07026694,   -6.20726496],\n",
       "       [  -5.51832733,   -4.51235501,   -6.60632339,   -5.75684708],\n",
       "       [  -5.03506954,   -3.88867603,   -6.0789793 ,   -5.39383027],\n",
       "       [  -4.4900702 ,   -3.02178232,   -5.37142048,   -4.49294031],\n",
       "       [  -3.77427957,   -3.18355583,   -1.94095892,   -3.81330039],\n",
       "       [  -8.45859578,  -10.20216019,   -8.80929146,   -8.6063284 ],\n",
       "       [  -8.2355623 ,  -12.73824027,  -80.4829465 ,   -9.09944414],\n",
       "       [  -7.79179207,   -9.8778365 ,  -72.62243123,   -8.14190648],\n",
       "       [  -7.42337053,   -7.36367475,  -68.64738137,   -9.37565393],\n",
       "       [  -7.1422298 ,   -7.1432981 ,  -54.69252563,  -10.83966256],\n",
       "       [  -7.11924041,  -10.82121819,  -60.29332089,   -8.39621159],\n",
       "       [  -6.05459127,   -6.04574107,  -54.91479083,   -6.06763183],\n",
       "       [  -5.63453758,   -6.40985163,  -19.58968502,   -5.73405262],\n",
       "       [  -5.14848599,   -6.00493818,  -49.34440965,   -5.18022888],\n",
       "       [  -4.32475677,   -7.4552641 ,  -60.56076376,   -6.2555877 ],\n",
       "       [  -3.58544536,   -1.90963232,  -79.88428633,   -3.19849581],\n",
       "       [  -3.12138267,   -2.03398844,   -1.        ,   -4.89566982],\n",
       "       [  -8.64072516, -108.63688974,  -20.48687347,  -10.38919668],\n",
       "       [   0.        ,    0.        ,    0.        ,    0.        ],\n",
       "       [   0.        ,    0.        ,    0.        ,    0.        ],\n",
       "       [   0.        ,    0.        ,    0.        ,    0.        ],\n",
       "       [   0.        ,    0.        ,    0.        ,    0.        ],\n",
       "       [   0.        ,    0.        ,    0.        ,    0.        ],\n",
       "       [   0.        ,    0.        ,    0.        ,    0.        ],\n",
       "       [   0.        ,    0.        ,    0.        ,    0.        ],\n",
       "       [   0.        ,    0.        ,    0.        ,    0.        ],\n",
       "       [   0.        ,    0.        ,    0.        ,    0.        ],\n",
       "       [   0.        ,    0.        ,    0.        ,    0.        ],\n",
       "       [   0.        ,    0.        ,    0.        ,    0.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finding the appropriate hyperparameters for Sarsa we can see that unlike Q_learning , it choose the safest root to Goal . Reason of this is that Sarsa is on_policy which make it more focused on the big punishments so it will try to avoid them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that Sarsa is more sensitive to its hyperparameters than other algorithms we seen . The reason that i belived that is i tried many combinations of hyperparameters and only with this one i get the optimal policy : \n",
    "gamma = 0.9\n",
    "alpha = 0.1\n",
    "epsilon = 0.1\n",
    "episodes = 5000\n",
    "time_limit = 100\n",
    "epsilon_decay = False\n",
    "\n",
    "obviously there are other combination of hyperparameters that work but i believe its harder to find them for Sarsa than other algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sarsa vs Q_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Risk : Sarsa tend to choose the safest path while Q_learning choose the shortest path.\n",
    "Policy Update : Sarsa updates are based on current policy's action but Q_learning updates are based on the optimal future actions.\n",
    "Exploration : Q_learning Explore based on the maximum reward potential that reslut in more exploitation of risky paths."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
