{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env,gamma,theta):\n",
    "    V = np.zeros((env.observation_space.n))\n",
    "\n",
    "    env.reset()\n",
    "    counter = 0\n",
    "    while (True):\n",
    "        counter += 1\n",
    "        delta = 0\n",
    "        for state in range(env.observation_space.n):\n",
    "            values = []\n",
    "            for action in range(env.action_space.n):\n",
    "                for probability ,next_state,reward, terminated in env.P[state][action]:\n",
    "                    values.append(probability * (reward + gamma * V[next_state]))\n",
    "                \n",
    "            best_value = np.max(values)\n",
    "                \n",
    "            if terminated : env.reset()\n",
    "                            \n",
    "            delta = max(delta,np.abs(V[state]-best_value))\n",
    "            V[state] = best_value\n",
    "\n",
    "        if delta < theta : break\n",
    "    env.close()\n",
    "    print(f'counter:{counter}')\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy(env,gamma,V):\n",
    "    policy = np.zeros(env.observation_space.n, dtype=int)\n",
    "    for state in range(env.observation_space.n):\n",
    "        values = []\n",
    "        for action in range(env.action_space.n):\n",
    "            for probability ,next_state,reward, terminated in env.P[state][action]:\n",
    "                values.append(probability * (reward + gamma * V[next_state]))\n",
    "\n",
    "        max_index = []\n",
    "        best_value = np.max(values)\n",
    "        for action in range(env.action_space.n):\n",
    "            if values[action] == best_value :\n",
    "                max_index.append(action)\n",
    "        if max_index != []:\n",
    "            policy[state] = np.random.choice(max_index)\n",
    "        else:\n",
    "            policy[state] = np.random.randint(0,4)\n",
    "            \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(theta,gamma,slippery):\n",
    "    env = gym.make('FrozenLake-v1',map_name='4x4',render_mode=None,is_slippery=slippery)\n",
    "    V  = value_iteration(env,gamma,theta)\n",
    "    policy = get_policy(env,gamma,V)\n",
    "    return V,policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk(policy,slippery):\n",
    "    env = gym.make('FrozenLake-v1',map_name='4x4',render_mode=\"human\",is_slippery=slippery)\n",
    "    state = env.reset()\n",
    "    terminated = False\n",
    "    state = 0\n",
    "    while not terminated:\n",
    "        action = policy[state]\n",
    "        next_state , reward, terminated, truncated , _ = env.step(action)\n",
    "        state = next_state \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value Iteration when is_slippery = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "low gamma\n",
    "gamma = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter:6\n",
      "Optimal_Value : [1.e-05 1.e-04 1.e-03 1.e-04 1.e-04 0.e+00 1.e-02 0.e+00 1.e-03 1.e-02\n",
      " 1.e-01 0.e+00 0.e+00 1.e-01 1.e+00 0.e+00] , Optimal_Policy : [1 2 1 0 1 0 1 3 2 2 1 3 3 2 2 0]\n"
     ]
    }
   ],
   "source": [
    "V , policy = train(theta=0.0001,gamma=0.1,slippery=False)\n",
    "print(f\"Optimal_Value : {V} , Optimal_Policy : {policy}\")\n",
    "walk(policy=policy,slippery=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that using gamma = 0.1 or low gamma , we will have convergence to optimal value if we have convergence rate(theta) below 0.001 . for higher value of theta the convergence is not guranteed . but we can see that if we use proper theta value we will converge quickly to the optimal value.\n",
    "also its obvious that the value function is much more focused on immediate rewards that could make big differenc  in environment with various type of rewards . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mid gamma\n",
    "gamma = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter:7\n",
      "Optimal_Value : [0.03125 0.0625  0.125   0.0625  0.0625  0.      0.25    0.      0.125\n",
      " 0.25    0.5     0.      0.      0.5     1.      0.     ] , Optimal_Policy : [2 2 1 0 1 1 1 0 2 1 1 0 2 2 2 0]\n"
     ]
    }
   ],
   "source": [
    "V , policy = train(theta=0.0001,gamma=0.5,slippery=False)\n",
    "print(f\"Optimal_Value : {V} , Optimal_Policy : {policy}\")\n",
    "walk(policy=policy,slippery=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most noticable thing is that with mid level gamma we can set the theta to 0.1 which is much bigger that the theta rate with low gamma .but if we use the same theta as last part we can see that this gamma converge to optimal value slower. also the 0.5 gamma means we focus equally on discounted and immediate reward ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High gamma\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter:7\n",
      "Optimal_Value : [0.59049 0.6561  0.729   0.6561  0.6561  0.      0.81    0.      0.729\n",
      " 0.81    0.9     0.      0.      0.9     1.      0.     ] , Optimal_Policy : [1 2 1 0 1 3 1 0 2 2 1 0 2 2 2 1]\n"
     ]
    }
   ],
   "source": [
    "V , policy = train(theta=0.0001,gamma=0.9,slippery=False)\n",
    "print(f\"Optimal_Value : {V} , Optimal_Policy : {policy}\")\n",
    "walk(policy=policy,slippery=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuslts for gamma = 0.9 is like results with gamma = 0.5 but theta rate could be even higher(7X higher) .We can see that the convegence speed is slower . The main difference is that with high gamma we are highly focused on rewards from future and immediate rewards are replaced in time . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value Iteration when is_slippery = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the stochastic environment we can see that agent learn things differently. If it's trained enough it wont even go near the holes beacuse there is chance it get stuck . So we can see the policy that it learn in slippery mode is very different from non_slippery mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "low gamma\n",
    "gamma = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter:3\n",
      "Optimal_Value : [0.         0.         0.         0.         0.         0.\n",
      " 0.00037037 0.         0.         0.00037037 0.01111111 0.\n",
      " 0.         0.01111111 0.33333333 0.        ] , Optimal_Policy : [0 1 2 3 0 3 2 3 1 2 2 2 0 3 2 3]\n"
     ]
    }
   ],
   "source": [
    "V , policy = train(theta=0.001,gamma=0.1,slippery=True)\n",
    "print(f\"Optimal_Value : {V} , Optimal_Policy : {policy}\")\n",
    "walk(policy=policy,slippery=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With low gamma ,  its hard for agent to get the actual value of each state that is connected to the Goal state . This happens beacuse due to low gamma , its very unlikly to get the real value of states that come from states in far future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mid gamma \n",
    "gamma  = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter:6\n",
      "Optimal_Value : [4.28669410e-05 2.57201646e-04 1.54320988e-03 2.57201646e-04\n",
      " 2.57201646e-04 0.00000000e+00 9.25925926e-03 0.00000000e+00\n",
      " 1.54320988e-03 9.25925926e-03 5.55555556e-02 0.00000000e+00\n",
      " 0.00000000e+00 5.55555556e-02 3.33333333e-01 0.00000000e+00] , Optimal_Policy : [2 3 2 3 2 2 2 2 3 2 2 2 2 3 1 3]\n"
     ]
    }
   ],
   "source": [
    "V , policy = train(theta=0.0001,gamma=0.5,slippery=True)\n",
    "print(f\"Optimal_Value : {V} , Optimal_Policy : {policy}\")\n",
    "walk(policy=policy,slippery=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that with looking more into future the agent can learn to avoid holes better . but still it's difficult to model this stochastic env."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High gamma \n",
    "gamma =0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter:7\n",
      "Optimal_Value : [0.00081    0.0027     0.009      0.0027     0.0027     0.\n",
      " 0.03       0.         0.009      0.03       0.1        0.\n",
      " 0.         0.1        0.33333333 0.        ] , Optimal_Policy : [2 0 2 3 2 2 2 1 3 2 2 1 1 1 1 2]\n"
     ]
    }
   ],
   "source": [
    "V , policy = train(theta=0.0001,gamma=0.9,slippery=True)\n",
    "print(f\"Optimal_Value : {V} , Optimal_Policy : {policy}\")\n",
    "walk(policy=policy,slippery=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that with high gamma , the value of the states near the goal are very high and the agent will try to reach them faster . unlike this we saw in last two parts that the value of the states around goal are less beacuse lower gammas do not look that deep into future state of this states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Iteration vs Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance of both algorithms varies based on the discount factor and the evironment's stochatic property . \n",
    "Policy iteration is faster due to variating between policies . \n",
    "Value iteration seems to be more robust in stochastic environment . "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
