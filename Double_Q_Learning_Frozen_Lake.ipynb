{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gamma = 0.9\\nalpha = 0.85\\nepsilon = 0.8\\nepisodes = 1000\\ntime_limit = 100'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "def epsilon_greedy(env,state,epsilon,q1,q2):\n",
    "    if np.random.default_rng().random() < epsilon : \n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        values = np.array([q1[(state,a)] + q2[(state,a)] for a in range(env.action_space.n)])\n",
    "        action = np.argmax(values)\n",
    "        return action\n",
    "    \n",
    "def double_q_learning(env,q1,q2,episodes,time_limit,gamma,epsilon,alpha,epsilon_decay):\n",
    "\n",
    "\n",
    "    for i in range(episodes):\n",
    "        state , _ = env.reset()\n",
    "        \n",
    "        truncated = False\n",
    "        terminated = False\n",
    "        time_step = 0\n",
    "\n",
    "        while not terminated and not truncated:\n",
    "            action = epsilon_greedy(env,state,epsilon,q1,q2)\n",
    "            next_state , reward , terminated , truncated , _ = env.step(action)\n",
    "            \n",
    "            if np.random.default_rng().random() < 0.5 : \n",
    "                next_action = np.argmax([q1[(next_state,a)] for a in range(env.action_space.n)])\n",
    "                q1[(state,action)] += alpha * (reward + gamma * q2[(next_state,next_action)] - q1[(state , action)])\n",
    "            else:\n",
    "                next_action = np.argmax([q2[(next_state,a)] for a in range(env.action_space.n)])\n",
    "                q2[(state,action)] += alpha * (reward + gamma * q1[(next_state,next_action)] - q2[(state , action)])\n",
    "            \n",
    "            state = next_state\n",
    "\n",
    "            time_step += 1 \n",
    "            if time_step > time_limit:\n",
    "                truncated = True\n",
    "                \n",
    "        if epsilon > 0 and epsilon_decay :\n",
    "            epsilon -= 0.001\n",
    "\n",
    "    return q1,q2\n",
    "\n",
    "\n",
    "def walk(q_,slippery):\n",
    "    env = gym.make(\"FrozenLake-v1\" , map_name='4x4' , is_slippery=slippery , render_mode='human')\n",
    "    state,_ = env.reset()\n",
    "    terminated = False\n",
    "    step = 60\n",
    "    q = q_\n",
    "    while not terminated:\n",
    "        max = -10000000000\n",
    "        for a in range(env.action_space.n):\n",
    "            if max < q[(state,a)]:\n",
    "                max = q[(state,a)]\n",
    "                action = a\n",
    "\n",
    "        next_state , reward, terminated, truncated , _ = env.step(action)\n",
    "        state = next_state \n",
    "        step-=1\n",
    "        if step == 0 : break\n",
    "    env.close()\n",
    "\n",
    "\n",
    "'''gamma = 0.9\n",
    "alpha = 0.85\n",
    "epsilon = 0.8\n",
    "episodes = 1000\n",
    "time_limit = 100'''\n",
    "# alpha = 0.5\n",
    "# gamma = 0.9\n",
    "# epsilon = 1\n",
    "# episodes = 10000\n",
    "# time_limit = 100\n",
    "\n",
    "# env = gym.make(\"FrozenLake-v1\" , map_name='4x4' , is_slippery=False , render_mode=None)\n",
    "\n",
    "# q1 = np.zeros((env.observation_space.n,env.action_space.n),dtype=float)\n",
    "# q2 = np.zeros((env.observation_space.n,env.action_space.n),dtype=float)\n",
    "\n",
    "# q1,q2 = double_q_learning(env,episodes,time_limit,gamma,epsilon,alpha,epsilon_decay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(time_limit,episodes,epsilon,gamma,alpha,slippery):\n",
    "    env = gym.make(\"FrozenLake-v1\" , map_name='4x4' , is_slippery=slippery , render_mode=None)\n",
    "    q1 = np.zeros((env.observation_space.n,env.action_space.n),dtype=float)\n",
    "    q2 = np.zeros((env.observation_space.n,env.action_space.n),dtype=float)    \n",
    "    q1_,q2_ = double_q_learning(env,q1,q2,episodes,time_limit,gamma,epsilon,alpha,epsilon_decay=True)    \n",
    "    walk(q1_,slippery)\n",
    "    return q1_,q2_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double Q_Learning non Slippery mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.14977343, 0.5903531 , 0.08999043, 0.23358923],\n",
       "       [0.22061783, 0.        , 0.        , 0.00156552],\n",
       "       [0.00125805, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.1481859 , 0.65596767, 0.        , 0.18571382],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.32967254, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.19099169, 0.        , 0.72898372, 0.18575165],\n",
       "       [0.23080193, 0.489798  , 0.80998831, 0.        ],\n",
       "       [0.28132771, 0.89999987, 0.        , 0.00728833],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.04890997, 0.73335008, 0.07215706],\n",
       "       [0.22424271, 0.61183752, 0.99999995, 0.50962905],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1,q2 = run(time_limit=100,episodes=1000,epsilon=1,gamma=0.9,alpha=0.1,slippery=False)\n",
    "q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.531441, 0.59049 , 0.59049 , 0.531441],\n",
       "       [0.531441, 0.      , 0.6561  , 0.59049 ],\n",
       "       [0.59049 , 0.729   , 0.59049 , 0.6561  ],\n",
       "       [0.6561  , 0.      , 0.59049 , 0.59049 ],\n",
       "       [0.59049 , 0.6561  , 0.      , 0.531441],\n",
       "       [0.      , 0.      , 0.      , 0.      ],\n",
       "       [0.      , 0.81    , 0.      , 0.6561  ],\n",
       "       [0.      , 0.      , 0.      , 0.      ],\n",
       "       [0.6561  , 0.      , 0.729   , 0.59049 ],\n",
       "       [0.6561  , 0.81    , 0.81    , 0.      ],\n",
       "       [0.729   , 0.9     , 0.      , 0.729   ],\n",
       "       [0.      , 0.      , 0.      , 0.      ],\n",
       "       [0.      , 0.      , 0.      , 0.      ],\n",
       "       [0.      , 0.81    , 0.9     , 0.729   ],\n",
       "       [0.81    , 0.9     , 1.      , 0.81    ],\n",
       "       [0.      , 0.      , 0.      , 0.      ]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1,q2 = run(time_limit=100,episodes=1000,epsilon=1,gamma=0.9,alpha=1,slippery=False)\n",
    "q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that like Q_learning , alpha is controling the size of the steps that change our values. for this small environment we can set different alpha's and still converge to optimal value but for more complex problems we must select a proper alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.66997937e-07, 9.99980994e-06, 3.68334330e-08, 7.32446355e-07],\n",
       "       [5.64498733e-07, 0.00000000e+00, 1.02803801e-15, 3.24842578e-12],\n",
       "       [1.29241033e-09, 0.00000000e+00, 0.00000000e+00, 3.79349821e-16],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [6.43062457e-06, 9.99991173e-05, 0.00000000e+00, 7.32669687e-07],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 1.37654810e-03, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [6.33453649e-05, 0.00000000e+00, 9.99999472e-04, 7.59499538e-06],\n",
       "       [6.30484273e-05, 9.99999445e-03, 4.41234580e-03, 0.00000000e+00],\n",
       "       [2.19488171e-04, 9.50647168e-02, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 6.66751371e-03, 9.99999990e-02, 8.25132602e-04],\n",
       "       [8.53691256e-03, 7.76908740e-02, 9.99999997e-01, 5.53950085e-03],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1,q2 = run(time_limit=100,episodes=1000,epsilon=1,gamma=0.1,alpha=0.1,slippery=False)\n",
    "q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.41756759e-02, 3.12499592e-02, 1.20478549e-02, 1.40674184e-02],\n",
       "       [4.15127687e-03, 0.00000000e+00, 2.92242386e-02, 8.19721325e-04],\n",
       "       [1.10660270e-03, 9.76626632e-02, 2.46285262e-05, 6.40789492e-03],\n",
       "       [6.15697003e-05, 0.00000000e+00, 0.00000000e+00, 1.25366999e-06],\n",
       "       [2.64035430e-02, 6.24999737e-02, 0.00000000e+00, 1.31795628e-02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 2.37877676e-01, 0.00000000e+00, 1.09287059e-02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [4.49134505e-02, 0.00000000e+00, 1.24999994e-01, 2.08210485e-02],\n",
       "       [4.59833171e-02, 1.82771232e-01, 2.49999998e-01, 0.00000000e+00],\n",
       "       [9.68083415e-02, 4.99999999e-01, 0.00000000e+00, 9.38462914e-02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 7.79641470e-02, 4.70249241e-01, 4.07488310e-02],\n",
       "       [1.68384223e-01, 4.05040427e-01, 1.00000000e+00, 2.31562529e-01],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1,q2 = run(time_limit=100,episodes=1000,epsilon=1,gamma=0.5,alpha=0.1,slippery=False)\n",
    "q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.30242448e-01, 5.90469794e-01, 1.75667952e-01, 3.30759287e-01],\n",
       "       [3.68203878e-01, 0.00000000e+00, 6.02828819e-04, 2.25388019e-02],\n",
       "       [4.29856536e-02, 0.00000000e+00, 0.00000000e+00, 6.00329148e-04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [4.80278010e-01, 6.56085992e-01, 0.00000000e+00, 3.58030060e-01],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 2.71047547e-01, 0.00000000e+00, 6.03551938e-04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [4.58109299e-01, 0.00000000e+00, 7.28998417e-01, 4.51306537e-01],\n",
       "       [3.88228796e-01, 4.30919087e-01, 8.09999164e-01, 0.00000000e+00],\n",
       "       [4.27709101e-01, 8.99999971e-01, 0.00000000e+00, 9.41940541e-02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 1.37325062e-01, 8.41358273e-01, 2.43370885e-01],\n",
       "       [3.77064081e-01, 6.06865623e-01, 9.99999995e-01, 6.44584431e-01],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1,q2 = run(time_limit=100,episodes=1000,epsilon=1,gamma=0.9,alpha=0.1,slippery=False)\n",
    "q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see with higher value of gamma we will look further into future and also it is controling the magnitude of our values or we could say with higher gammas , large rewards from future play a rule in the values of the current states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double Q_Learning Slippery mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.24968650e-02, 0.00000000e+00, 6.76688091e-04, 4.25043734e-04],\n",
       "       [1.01074956e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.79134708e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [2.79223846e-02, 7.89522318e-04, 1.80289346e-03, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [4.22721143e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [4.41071160e-04, 3.46017008e-04, 0.00000000e+00, 4.52745120e-02],\n",
       "       [3.84431441e-02, 8.57365240e-03, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.16144455e-01, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 6.56100000e-03, 2.33248907e-01, 4.39298124e-02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 6.56100000e-03, 3.96852600e-01],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1,q2 = run(time_limit=100,episodes=10000,epsilon=1,gamma=0.9,alpha=0.1,slippery=True)\n",
    "q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.31551656e-03, 3.38161458e-03, 1.81072654e-03, 1.44417524e-03],\n",
       "       [1.44657510e-03, 1.01804753e-03, 8.09038624e-03, 2.98154493e-03],\n",
       "       [4.77692886e-03, 4.40401563e-03, 2.08560534e-02, 1.97508780e-03],\n",
       "       [1.59177928e-03, 2.02575598e-03, 2.16124646e-03, 1.29855266e-02],\n",
       "       [1.01882751e-04, 7.69343775e-04, 2.35733415e-03, 9.96002285e-05],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.97736994e-03, 0.00000000e+00, 2.28653880e-02, 6.15748384e-04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.28324265e-04, 1.20917480e-04, 3.39563745e-03, 1.07347299e-02],\n",
       "       [3.31862949e-05, 0.00000000e+00, 0.00000000e+00, 1.52645838e-02],\n",
       "       [6.37048235e-02, 0.00000000e+00, 1.84866560e-01, 1.45615927e-03],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 3.64348876e-02, 8.53409892e-03, 2.91212771e-02],\n",
       "       [0.00000000e+00, 5.18928735e-01, 1.71551937e-01, 9.23585602e-02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1,q2 = run(time_limit=100,episodes=1000,epsilon=1,gamma=0.9,alpha=0.1,slippery=True)\n",
    "q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.75185956e-02, 1.40973887e-03, 1.61451559e-03, 1.64373868e-04],\n",
       "       [1.36179882e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [2.95341269e-02, 4.59504177e-04, 3.14533867e-03, 4.52772607e-04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [3.11919114e-03, 7.31410627e-04, 1.71017906e-03, 5.50594128e-02],\n",
       "       [9.27320010e-02, 0.00000000e+00, 1.79555744e-03, 0.00000000e+00],\n",
       "       [5.31441000e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [3.20259940e-03, 0.00000000e+00, 3.05545738e-01, 0.00000000e+00],\n",
       "       [0.00000000e+00, 5.07985328e-01, 9.00000000e-02, 9.00000000e-02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1,q2 = run(time_limit=100,episodes=50000,epsilon=1,gamma=0.9,alpha=0.1,slippery=True)\n",
    "q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like every other algorithm when we introduce uncerteity to our environment the problem become more complex . in the results we can see Double Q_learning can find the optimal q_values but it need more time(episodes) and hyperparameters must be selected with more precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double Q_learning vs Q_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance : both Q_learning and Double Q_learning perfrom well and converge to optimal policy . \n",
    "Convergence Speed : Q_learning is fast and stable but Double q_learning is slightly slower due to the need to update two separate value functions.\n",
    "Maxization Bias : the main reason of using Double Q_learning was to control maximization bias and it fix the problem . \n",
    "\n",
    "But in the slippery mode we see some changes in the performance of this algorithms. \n",
    "Q_learning performance degrade and become slower and unstable while Double Q_learning show improved speed and performance due to reducing overestimation bias."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
