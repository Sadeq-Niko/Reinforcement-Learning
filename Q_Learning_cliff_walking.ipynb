{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "def epsilon_greedy(state,epsilon):\n",
    "    if np.random.default_rng().random() < epsilon : \n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        \n",
    "        return max(list(range(env.action_space.n)),key = lambda x: q[(state,x)])\n",
    "    \n",
    "def q_learning(env,episodes,time_limit,gamma,epsilon,alpha,epsilon_decay):\n",
    "    \n",
    "    for i in range(episodes):\n",
    "        state , _ = env.reset()\n",
    "        \n",
    "        truncated = False\n",
    "        terminated = False\n",
    "        time_step = 0\n",
    "\n",
    "        while not terminated and not truncated:\n",
    "            action = epsilon_greedy(state,epsilon)\n",
    "            next_state , reward , terminated , truncated , _ = env.step(action)\n",
    "\n",
    "            next_action = np.argmax([q[(next_state,a)] for a in range(env.action_space.n)])\n",
    "            q[(state,action)] += alpha * (reward + gamma * q[(next_state,next_action)] - q[(state , action)])\n",
    "            \n",
    "            state = next_state\n",
    "\n",
    "            time_step += 1 \n",
    "            if time_step > time_limit:\n",
    "                truncated = True\n",
    "                \n",
    "        if epsilon > 0 and epsilon_decay :\n",
    "            epsilon -= 0.001\n",
    "\n",
    "    return q\n",
    "\n",
    "\n",
    "def walk(q):\n",
    "    env = gym.make('CliffWalking-v0', render_mode='human')\n",
    "    state,_ = env.reset()\n",
    "    terminated = False\n",
    "    step = 60\n",
    "    while not terminated:\n",
    "        max = -10000000000\n",
    "        for a in range(env.action_space.n):\n",
    "            if max < q[(state,a)]:\n",
    "                max = q[(state,a)]\n",
    "                action = a\n",
    "\n",
    "        next_state , reward, terminated, truncated , _ = env.step(action)\n",
    "        state = next_state \n",
    "        step-=1\n",
    "        if step == 0 : break\n",
    "    env.close()\n",
    "\n",
    "\n",
    "'''gamma = 0.9\n",
    "alpha = 0.85\n",
    "epsilon = 0.8\n",
    "episodes = 1000\n",
    "time_limit = 100'''\n",
    "alpha = 0.85\n",
    "gamma = 0.9\n",
    "epsilon = 0.8\n",
    "episodes = 1000\n",
    "time_limit = 100\n",
    "\n",
    "env = gym.make('CliffWalking-v0', render_mode=None)\n",
    "\n",
    "q = np.zeros((env.observation_space.n,env.action_space.n),dtype=float)\n",
    "\n",
    "q_ = q_learning(env,episodes,time_limit,gamma,epsilon,alpha,epsilon_decay=True)\n",
    "\n",
    "# walk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk(q_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  -7.94108868,   -7.71232075,   -7.71232075,   -7.94108868],\n",
       "       [  -7.71232075,   -7.45813417,   -7.45813417,   -7.94108868],\n",
       "       [  -7.45813417,   -7.17570464,   -7.17570464,   -7.71232075],\n",
       "       [  -7.17570464,   -6.86189404,   -6.86189404,   -7.45813417],\n",
       "       [  -6.86189404,   -6.5132156 ,   -6.5132156 ,   -7.17570464],\n",
       "       [  -6.5132156 ,   -6.12579511,   -6.12579511,   -6.86189404],\n",
       "       [  -6.12579511,   -5.6953279 ,   -5.6953279 ,   -6.5132156 ],\n",
       "       [  -5.6953279 ,   -5.217031  ,   -5.217031  ,   -6.12579511],\n",
       "       [  -5.217031  ,   -4.68559   ,   -4.68559   ,   -5.6953279 ],\n",
       "       [  -4.68559   ,   -4.0951    ,   -4.0951    ,   -5.217031  ],\n",
       "       [  -4.0951    ,   -3.439     ,   -3.439     ,   -4.68559   ],\n",
       "       [  -3.439     ,   -3.439     ,   -2.71      ,   -4.0951    ],\n",
       "       [  -7.94108868,   -7.45813417,   -7.45813417,   -7.71232075],\n",
       "       [  -7.71232075,   -7.17570464,   -7.17570464,   -7.71232075],\n",
       "       [  -7.45813417,   -6.86189404,   -6.86189404,   -7.45813417],\n",
       "       [  -7.17570464,   -6.5132156 ,   -6.5132156 ,   -7.17570464],\n",
       "       [  -6.86189404,   -6.12579511,   -6.12579511,   -6.86189404],\n",
       "       [  -6.5132156 ,   -5.6953279 ,   -5.6953279 ,   -6.5132156 ],\n",
       "       [  -6.12579511,   -5.217031  ,   -5.217031  ,   -6.12579511],\n",
       "       [  -5.6953279 ,   -4.68559   ,   -4.68559   ,   -5.6953279 ],\n",
       "       [  -5.217031  ,   -4.0951    ,   -4.0951    ,   -5.217031  ],\n",
       "       [  -4.68559   ,   -3.439     ,   -3.439     ,   -4.68559   ],\n",
       "       [  -4.0951    ,   -2.71      ,   -2.71      ,   -4.0951    ],\n",
       "       [  -3.439     ,   -2.71      ,   -1.9       ,   -3.439     ],\n",
       "       [  -7.71232075,   -7.17570464,   -7.71232075,   -7.45813417],\n",
       "       [  -7.45813417,   -6.86189404, -106.71232075,   -7.45813417],\n",
       "       [  -7.17570464,   -6.5132156 , -106.71232075,   -7.17570464],\n",
       "       [  -6.86189404,   -6.12579511, -106.71232075,   -6.86189404],\n",
       "       [  -6.5132156 ,   -5.6953279 , -106.71232075,   -6.5132156 ],\n",
       "       [  -6.12579511,   -5.217031  , -106.71232075,   -6.12579511],\n",
       "       [  -5.6953279 ,   -4.68559   , -106.71232075,   -5.6953279 ],\n",
       "       [  -5.217031  ,   -4.0951    , -106.71232075,   -5.217031  ],\n",
       "       [  -4.68559   ,   -3.439     , -106.71232075,   -4.68559   ],\n",
       "       [  -4.0951    ,   -2.71      , -106.71232075,   -4.0951    ],\n",
       "       [  -3.439     ,   -1.9       , -106.71232075,   -3.439     ],\n",
       "       [  -2.71      ,   -1.9       ,   -1.        ,   -2.71      ],\n",
       "       [  -7.45813417, -106.71232075,   -7.71232075,   -7.71232075],\n",
       "       [   0.        ,    0.        ,    0.        ,    0.        ],\n",
       "       [   0.        ,    0.        ,    0.        ,    0.        ],\n",
       "       [   0.        ,    0.        ,    0.        ,    0.        ],\n",
       "       [   0.        ,    0.        ,    0.        ,    0.        ],\n",
       "       [   0.        ,    0.        ,    0.        ,    0.        ],\n",
       "       [   0.        ,    0.        ,    0.        ,    0.        ],\n",
       "       [   0.        ,    0.        ,    0.        ,    0.        ],\n",
       "       [   0.        ,    0.        ,    0.        ,    0.        ],\n",
       "       [   0.        ,    0.        ,    0.        ,    0.        ],\n",
       "       [   0.        ,    0.        ,    0.        ,    0.        ],\n",
       "       [   0.        ,    0.        ,    0.        ,    0.        ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at q_values calculated by q_learning we can see that after setting proper hyper parameters it achive the q_values(wich can policy could be extracted from) that walk as closly as possible to valley . reason of this act is that q_learning is off_policy wich make it take a more risky way toward the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sarsa vs Q_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Risk : Sarsa tend to choose the safest path while Q_learning choose the shortest path.\n",
    "Policy Update : Sarsa updates are based on current policy's action but Q_learning updates are based on the optimal future actions.\n",
    "Exploration : Q_learning Explore based on the maximum reward potential that reslut in more exploitation of risky paths."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
